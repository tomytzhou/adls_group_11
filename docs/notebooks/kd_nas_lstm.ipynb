{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MWXKotO1y4Ga"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    AutoModelForMaskedLM,\n",
        "    AdamW\n",
        ")\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i8-YIDB7y-9_"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"albert/albert-base-v2\"\n",
        "tokenizer_checkpoint = \"albert/albert-base-v2\"\n",
        "dataset_name = \"xu-song/cc100-samples\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dc1f6764876b43ceb68a6ab0a1e312bd",
            "9b7ffcfcfc6646a3ab6ef8f149bca22f",
            "3dec93f7107c4a0482b55fc5b6b49075",
            "ccce72945cf8409e9b0587f62eb74a58",
            "2846cdfca175461ebfe3bbf94e1ad56e",
            "187339b4a8d84bfe876a536bed761bff",
            "5086ada160834376a44919a674efb0bd",
            "eba6c68faae34d6ea86049fbb47a29c8",
            "6d86f20665d54d1089b63eeccece63d4",
            "f2197858d207439abbe4eab1d290da7c",
            "3d21619401a34192aaedcb6484f9c916"
          ]
        },
        "id": "2wgUmCRzzDO_",
        "outputId": "b2a007ca-ec6e-4287-cc3e-3839d71db686"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc1f6764876b43ceb68a6ab0a1e312bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(dataset_name, \"en\", split=\"train[:100%]\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=50\n",
        "    )\n",
        "\n",
        "# Tokenize and split dataset\n",
        "dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "TCM-aEZT1tYp",
        "outputId": "420fdd20-14ed-4a4d-aeae-7f64d945f2e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40E_ao1n11ZP",
        "outputId": "a3e49d59-a1a3-4569-cef9-186baf842204"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
            "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Load teacher model and ensure it outputs hidden states.\n",
        "teacher_model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "teacher_model.config.output_hidden_states = True\n",
        "teacher_model.eval()  # Set teacher in evaluation mode\n",
        "for param in teacher_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3PqRcvWs2V3A"
      },
      "outputs": [],
      "source": [
        "# Define a small student configuration (BERT-like) for distillation.\n",
        "config = AutoConfig.from_pretrained(\n",
        "    pretrained_model_name_or_path=checkpoint,\n",
        "    hidden_size=128,\n",
        "    num_hidden_layers=2,\n",
        "    num_attention_heads=2,\n",
        "    intermediate_size=384,\n",
        "    hidden_act=\"gelu\",\n",
        ")\n",
        "student_model = AutoModelForMaskedLM.from_config(config)\n",
        "student_model.config.output_hidden_states = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7IQ4he2B2fCs"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGfZUDUb2iyJ"
      },
      "outputs": [],
      "source": [
        "def trainer(dataloader, teacher_model, student_model, epochs=10):\n",
        "    projection = torch.nn.Linear(student_model.config.hidden_size,\n",
        "                                 2 * teacher_model.config.hidden_size).to('cuda')\n",
        "\n",
        "    # Jointly optimise student model and projection - simplifcation compared to paper\n",
        "    optimizer = torch.optim.AdamW(list(student_model.parameters()) + list(projection.parameters()), lr=5e-5)\n",
        "\n",
        "    student_model.to('cuda')\n",
        "    teacher_model.to('cuda')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        student_model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to('cuda')\n",
        "            attention_mask = batch[\"attention_mask\"].to('cuda')\n",
        "\n",
        "            # Forward pass through teacher (with no gradients).\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(input_ids=input_ids,\n",
        "                                                attention_mask=attention_mask,\n",
        "                                                output_hidden_states=True)\n",
        "            student_outputs = student_model(input_ids=input_ids,\n",
        "                                            attention_mask=attention_mask,\n",
        "                                            output_hidden_states=True)\n",
        "\n",
        "            # Extract hidden states - skip embedding\n",
        "            teacher_hidden = teacher_outputs.hidden_states[1:] \n",
        "            student_hidden = student_outputs.hidden_states[1:]\n",
        "\n",
        "            # For each student layer, project the full hidden state\n",
        "            Hs = torch.stack([h for h in student_hidden])  # shape: (num_student_layers, batch, seq_len, hidden_size)\n",
        "            Hs_proj = projection(Hs)  # shape: (num_student_layers, batch, seq_len, 2 * teacher_hidden_size)\n",
        "\n",
        "            # Map teacher hidden states to student layers using a Uniform+Last strategy\n",
        "            num_student_layers = len(student_hidden)\n",
        "            teacher_layers = teacher_hidden\n",
        "            num_teacher_layers = len(teacher_layers)\n",
        "            Ht = []\n",
        "            for i in range(num_student_layers):\n",
        "                # Uniform mapping index (adjust indices if using 0-indexing):\n",
        "                idx_uniform = int(i * num_teacher_layers / num_student_layers)\n",
        "                # Last mapping index:\n",
        "                idx_last = i + num_teacher_layers - num_student_layers\n",
        "\n",
        "                H0 = teacher_layers[idx_uniform]\n",
        "                H1 = teacher_layers[idx_last]\n",
        "                # Concatenate along the hidden dimension.\n",
        "                H_teacher_concat = torch.cat([H0, H1], dim=-1)  # shape: (batch, seq_len, 2*teacher_hidden_size)\n",
        "                Ht.append(H_teacher_concat)\n",
        "\n",
        "            # Stack teacher states to match student projection\n",
        "            Ht = torch.stack(Ht)  # shape: (num_student_layers, batch, seq_len, 2 * teacher_hidden_size)\n",
        "\n",
        "            loss = torch.nn.functional.mse_loss(Hs_proj, Ht)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1} | Avg Distillation Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Nl079fv2nRn",
        "outputId": "b8f88808-63d7-4775-834d-d0a2d2eab180"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Avg Distillation Loss: 0.6605\n",
            "Epoch 2 | Avg Distillation Loss: 0.4494\n",
            "Epoch 3 | Avg Distillation Loss: 0.4074\n",
            "Epoch 4 | Avg Distillation Loss: 0.3861\n",
            "Epoch 5 | Avg Distillation Loss: 0.3663\n",
            "Epoch 6 | Avg Distillation Loss: 0.3512\n",
            "Epoch 7 | Avg Distillation Loss: 0.3439\n",
            "Epoch 8 | Avg Distillation Loss: 0.3336\n",
            "Epoch 9 | Avg Distillation Loss: 0.3249\n",
            "Epoch 10 | Avg Distillation Loss: 0.3242\n"
          ]
        }
      ],
      "source": [
        "final_loss = trainer(train_dataloader, teacher_model, student_model, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CC09-1AoDpmc"
      },
      "outputs": [],
      "source": [
        "def get_search_space():\n",
        "    return {\n",
        "        \"num_hidden_layers\": [3, 4, 6, 10, 12],\n",
        "        \"num_attention_heads\": [2, 3, 4, 6, 12],\n",
        "        \"hidden_size\": [384, 768],\n",
        "        \"intermediate_size\": [384, 512, 576, 768, 1024, 1536, 2048, 3072],\n",
        "        \"hidden_act\": ['gelu', 'relu', 'silu']\n",
        "    }\n",
        "\n",
        "search_space = get_search_space()\n",
        "state_keys = list(search_space.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc0B8WHoDqxc"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertConfig, AlbertForMaskedLM\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Utils\n",
        "def construct_student_model_from_config(config):\n",
        "    new_config = AlbertConfig(**config)\n",
        "    model = AlbertForMaskedLM(new_config)\n",
        "    return model\n",
        "\n",
        "def get_latency(config_or_model_config):\n",
        "    \"\"\"\n",
        "    Measure average latency (in seconds) for a forward pass.\n",
        "    Use CPU as expected deployment on CPU and will probs get more stable results\n",
        "    \"\"\"\n",
        "    # construct model\n",
        "    if isinstance(config_or_model_config, dict):\n",
        "        model = construct_student_model_from_config(config_or_model_config)\n",
        "    else:\n",
        "        model = AlbertForMaskedLM(config_or_model_config)\n",
        "    model.eval()\n",
        "    model.to('cpu')\n",
        "\n",
        "    # dummy input\n",
        "    batch_size = 1\n",
        "    seq_length = 50\n",
        "    dummy_input = torch.randint(0, 100, (batch_size, seq_length))\n",
        "    attention_mask = torch.ones_like(dummy_input)\n",
        "\n",
        "    # warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):\n",
        "            _ = model(dummy_input, attention_mask=attention_mask)\n",
        "\n",
        "    num_runs = 20\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            _ = model(dummy_input, attention_mask=attention_mask)\n",
        "    end = time.time()\n",
        "    avg_latency = (end - start) / num_runs\n",
        "    return avg_latency\n",
        "\n",
        "def calculate_reward(loss, latency, teacher_latency, alpha=-0.06, beta=0.6**6):\n",
        "    \"\"\"\n",
        "    reward function from paper:\n",
        "    reward = (1 - L_HS) * (lat(S)/(beta * lat(T)))^alpha\n",
        "    \"\"\"\n",
        "    normalized_latency = latency / (beta * teacher_latency)\n",
        "    reward = (1 - loss) * (normalized_latency ** alpha)\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFYmSVlZPe-8"
      },
      "source": [
        "## LSTM Controller\n",
        "\n",
        "- **Architecture**: Embedding layers for each hyperparameter choice, followed by an LSTM, and a linear layer to predict a scalar reward.\n",
        "- **Input**: A sequence of three states (previous best, global best, current state), each represented as concatenated embeddings of hyperparameter indices.\n",
        "- **Output**: Predicted reward for the current state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "QGa4qtRrETeR"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# LSTM Controller\n",
        "class Controller(nn.Module):\n",
        "    def __init__(self, search_space, embedding_dim=32, lstm_hidden=32):\n",
        "        super(Controller, self).__init__()\n",
        "        self.embeddings = nn.ModuleDict()\n",
        "        for param, choices in search_space.items():\n",
        "            self.embeddings[param] = nn.Embedding(len(choices), embedding_dim)\n",
        "        input_size = embedding_dim * len(search_space)\n",
        "        self.lstm = nn.LSTM(input_size, lstm_hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(lstm_hidden, 1)\n",
        "        self.state_keys = list(search_space.keys())\n",
        "\n",
        "    def forward(self, prev_best, global_best, current_state):\n",
        "        \"\"\"Predict reward for current_state given prev_best and global_best.\"\"\"\n",
        "        prev_best_emb = torch.cat([self.embeddings[param](torch.tensor([idx], device='cuda'))\n",
        "                                 for param, idx in zip(self.state_keys, prev_best)], dim=-1)\n",
        "        global_best_emb = torch.cat([self.embeddings[param](torch.tensor([idx], device='cuda'))\n",
        "                                   for param, idx in zip(self.state_keys, global_best)], dim=-1)\n",
        "        current_state_emb = torch.cat([self.embeddings[param](torch.tensor([idx], device='cuda'))\n",
        "                                     for param, idx in zip(self.state_keys, current_state)], dim=-1)\n",
        "        sequence = torch.stack([prev_best_emb, global_best_emb, current_state_emb], dim=0)\n",
        "        _, (h_n, _) = self.lstm(sequence.permute(1, 0, 2))\n",
        "        reward_pred = self.fc(h_n[-1])\n",
        "        return reward_pred.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1xCzDkBPxh8"
      },
      "source": [
        "## Mini-KD Process\n",
        "- **Proxy Set**: 30% of the training data, shuffled and selected once before the NAS loop\n",
        "- **Training**: 4 epochs of KD, with hidden state mapping + projection\n",
        "- **Output**: Average distillation loss of final epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "NcmCGYf5EmvG"
      },
      "outputs": [],
      "source": [
        "def mini_kd_trainer(proxy_data, teacher_model, student_model, epochs=4):\n",
        "    \"\"\"Perform Mini-KD on proxy data and return average distillation loss.\"\"\"\n",
        "    proxy_dataloader = DataLoader(proxy_data, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
        "    projection = nn.Linear(student_model.config.hidden_size, 2 * teacher_model.config.hidden_size).to('cuda')\n",
        "\n",
        "    optimizer = torch.optim.AdamW(list(student_model.parameters()) + list(projection.parameters()), lr=5e-5)\n",
        "    student_model.to('cuda')\n",
        "    teacher_model.to('cuda')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        student_model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch in proxy_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to('cuda')\n",
        "            attention_mask = batch[\"attention_mask\"].to('cuda')\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "            student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "            teacher_hidden = teacher_outputs.hidden_states[1:]\n",
        "            student_hidden = student_outputs.hidden_states[1:]\n",
        "            Hs = torch.stack([h for h in student_hidden])\n",
        "            Hs_proj = projection(Hs)\n",
        "            num_student_layers = len(student_hidden)\n",
        "            num_teacher_layers = len(teacher_hidden)\n",
        "            Ht = []\n",
        "            for i in range(num_student_layers):\n",
        "                idx_uniform = int(i * num_teacher_layers / num_student_layers)\n",
        "                idx_last = i + num_teacher_layers - num_student_layers\n",
        "                H_teacher_concat = torch.cat([teacher_hidden[idx_uniform], teacher_hidden[idx_last]], dim=-1)\n",
        "                Ht.append(H_teacher_concat)\n",
        "            Ht = torch.stack(Ht)\n",
        "            loss = nn.functional.mse_loss(Hs_proj, Ht)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_loss = running_loss / len(proxy_dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Avg Distillation Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "6hR1XyBAFCoz"
      },
      "outputs": [],
      "source": [
        "def train_controller(controller, optimizer, training_data, prev_best, global_best, epochs=10):\n",
        "    \"\"\"Train the controller to predict rewards using MSE loss.\"\"\"\n",
        "    controller.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for state, reward in training_data:\n",
        "            prev_best_tensor = torch.tensor(prev_best, device='cuda')\n",
        "            global_best_tensor = torch.tensor(global_best, device='cuda')\n",
        "            state_tensor = torch.tensor(state, device='cuda')\n",
        "            reward_tensor = torch.tensor([reward], dtype=torch.float32, device='cuda')\n",
        "            optimizer.zero_grad()\n",
        "            reward_pred = controller(prev_best_tensor, global_best_tensor, state_tensor)\n",
        "            # loss = nn.functional.mse_loss(reward_pred, reward_tensor)\n",
        "            loss = nn.functional.mse_loss(reward_pred.unsqueeze(0), reward_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_loss = running_loss / len(training_data)\n",
        "    print(f\"Controller Epoch {epochs} | Avg Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WntO9qUXFOVT",
        "outputId": "454cf847-0e5c-4569-9bd8-5248c0e57dd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Hyperparams\n",
        "M = 10  # Number of episodes. Complete run = 15\n",
        "N = 5  # Number of candidates per episode. Complete run = 20\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.05\n",
        "pool_size = 100  # Size of state pool for controller selection\n",
        "\n",
        "# Initialise controller\n",
        "controller = Controller(search_space).to('cuda')\n",
        "controller_optimizer = torch.optim.RMSprop(controller.parameters(), lr=1e-4)\n",
        "\n",
        "# Initialise best states\n",
        "random_state = [np.random.randint(len(search_space[key])) for key in state_keys]\n",
        "global_best = {'state': random_state, 'reward': -float('inf')}\n",
        "previous_best = {'state': random_state, 'reward': -float('inf')}\n",
        "\n",
        "# Proxy dataset - 30% of original for shorter KD\n",
        "proxy_size = int(0.3 * len(train_data))\n",
        "proxy_data = train_data.shuffle().select(range(proxy_size))\n",
        "\n",
        "teacher_latency = get_latency(teacher_model.config)\n",
        "\n",
        "evaluated_states = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zphLzuc9QN98"
      },
      "source": [
        "## NAS Loop\n",
        "- **Episodes**: M episodes, with N candidates per episode. Controller trained at the end of every episode.\n",
        "- **Candidate Selection**: Controller samples a pool (e.g. 100 states), predicts rewards, and selects the top ones, random states fill the rest\n",
        "- **Exploration/Exploitation**: Controlled by Îµ, starting at 1.0 and decaying to 0.05. Later episodes use more samples from controller as it improves from training. See paper\n",
        "- **Evaluation**: Mini-KD provides loss and latency for reward calculation\n",
        "- **Controller Training**: Uses MSE loss between predicted and actual rewards, incorporating all candidate states plus global/previous best states\n",
        "\n",
        "TODO:\n",
        "- Cache architecture rewards, so if duplicate sample, then can just fetch reward and avoid KD again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50MZ3ISsFZ2C",
        "outputId": "1e4e8685-cf8a-479e-e9a6-62d8feb8cb6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Episode 1/10, Exploration Ratio: 1.00\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 4\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1024\n",
            "    hidden_act: gelu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8565\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7207\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5882\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5430\n",
            "    Mini-KD Loss: 0.5430, Latency: 0.0137, Reward: 0.4231\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 576\n",
            "    hidden_act: gelu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8406\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6801\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5752\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5433\n",
            "    Mini-KD Loss: 0.5433, Latency: 0.0161, Reward: 0.4188\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 384\n",
            "    hidden_act: gelu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8408\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6892\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5839\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5416\n",
            "    Mini-KD Loss: 0.5416, Latency: 0.0156, Reward: 0.4211\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 768\n",
            "    hidden_act: gelu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8577\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7142\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6373\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5727\n",
            "    Mini-KD Loss: 0.5727, Latency: 0.0172, Reward: 0.3903\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 4\n",
            "    num_attention_heads: 4\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 768\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8524\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7356\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6001\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5496\n",
            "    Mini-KD Loss: 0.5496, Latency: 0.0122, Reward: 0.4199\n",
            "Controller Epoch 1/10 | Avg Loss: 0.0280\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0034\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0015\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0007\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-c6a9045eda3c>:13: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(reward_pred, reward_tensor)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Controller Epoch 6/10 | Avg Loss: 0.0001\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0001\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0000\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0000\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0000\n",
            "\n",
            "Episode 2/10, Exploration Ratio: 0.95\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 768\n",
            "    hidden_act: gelu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9244\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7673\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7477\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.6742\n",
            "    Mini-KD Loss: 0.6742, Latency: 0.0115, Reward: 0.3048\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 3\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8372\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7284\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5953\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5195\n",
            "    Mini-KD Loss: 0.5195, Latency: 0.0120, Reward: 0.4484\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: gelu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7820\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6347\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5451\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5407\n",
            "    Mini-KD Loss: 0.5407, Latency: 0.0333, Reward: 0.4032\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 6\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 384\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9224\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7677\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7476\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.7204\n",
            "    Mini-KD Loss: 0.7204, Latency: 0.0089, Reward: 0.2657\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 4\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8555\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7196\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6845\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.6015\n",
            "    Mini-KD Loss: 0.6015, Latency: 0.0170, Reward: 0.3642\n",
            "Controller Epoch 1/10 | Avg Loss: 0.1231\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0381\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0177\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0098\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0061\n",
            "Controller Epoch 6/10 | Avg Loss: 0.0040\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0027\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0019\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0012\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0008\n",
            "\n",
            "Episode 3/10, Exploration Ratio: 0.90\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 4\n",
            "    num_attention_heads: 6\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 768\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8534\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7172\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5836\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5310\n",
            "    Mini-KD Loss: 0.5310, Latency: 0.0117, Reward: 0.4384\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 3\n",
            "    num_attention_heads: 4\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 2048\n",
            "    hidden_act: gelu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9220\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7744\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7529\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.7202\n",
            "    Mini-KD Loss: 0.7202, Latency: 0.0078, Reward: 0.2680\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 3072\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7704\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6991\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6950\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.6517\n",
            "    Mini-KD Loss: 0.6517, Latency: 0.0519, Reward: 0.2977\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 384\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8401\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7250\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5920\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5542\n",
            "    Mini-KD Loss: 0.5542, Latency: 0.0223, Reward: 0.4009\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 1024\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8464\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7180\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6684\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5882\n",
            "    Mini-KD Loss: 0.5882, Latency: 0.0184, Reward: 0.3746\n",
            "Controller Epoch 1/10 | Avg Loss: 0.0108\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0063\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0041\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0028\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0019\n",
            "Controller Epoch 6/10 | Avg Loss: 0.0014\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0010\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0008\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0006\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0005\n",
            "\n",
            "Episode 4/10, Exploration Ratio: 0.85\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 768\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8407\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7057\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5756\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5532\n",
            "    Mini-KD Loss: 0.5532, Latency: 0.0168, Reward: 0.4086\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 4\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 576\n",
            "    hidden_act: gelu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9433\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7769\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7576\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.7383\n",
            "    Mini-KD Loss: 0.7383, Latency: 0.0073, Reward: 0.2518\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 4\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 576\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8660\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7163\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6996\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.6258\n",
            "    Mini-KD Loss: 0.6258, Latency: 0.0150, Reward: 0.3447\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 6\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 576\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8577\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7187\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6900\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.6065\n",
            "    Mini-KD Loss: 0.6065, Latency: 0.0138, Reward: 0.3642\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 4\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 768\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9332\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7741\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7580\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.7449\n",
            "    Mini-KD Loss: 0.7449, Latency: 0.0074, Reward: 0.2451\n",
            "Controller Epoch 1/10 | Avg Loss: 0.0125\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0056\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0030\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0017\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0010\n",
            "Controller Epoch 6/10 | Avg Loss: 0.0006\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0004\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0002\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0002\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0001\n",
            "\n",
            "Episode 5/10, Exploration Ratio: 0.80\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 3\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 1024\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9382\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7740\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7547\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.7287\n",
            "    Mini-KD Loss: 0.7287, Latency: 0.0068, Reward: 0.2621\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 3\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 768\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8590\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7105\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5753\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5334\n",
            "    Mini-KD Loss: 0.5334, Latency: 0.0096, Reward: 0.4414\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 3\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 768\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9459\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7745\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7573\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.7450\n",
            "    Mini-KD Loss: 0.7450, Latency: 0.0061, Reward: 0.2478\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 384\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7934\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6308\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5670\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5230\n",
            "    Mini-KD Loss: 0.5230, Latency: 0.0279, Reward: 0.4233\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 4\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 768\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9276\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7675\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7506\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.7182\n",
            "    Mini-KD Loss: 0.7182, Latency: 0.0100, Reward: 0.2660\n",
            "Controller Epoch 1/10 | Avg Loss: 0.0097\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0057\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0041\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0032\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0025\n",
            "Controller Epoch 6/10 | Avg Loss: 0.0019\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0015\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0012\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0009\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0007\n",
            "\n",
            "Episode 6/10, Exploration Ratio: 0.75\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 3072\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7751\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6764\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5762\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5084\n",
            "    Mini-KD Loss: 0.5084, Latency: 0.0436, Reward: 0.4246\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 3072\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7687\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6844\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5892\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5309\n",
            "    Mini-KD Loss: 0.5309, Latency: 0.0435, Reward: 0.4053\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 512\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7827\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6424\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5741\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5374\n",
            "    Mini-KD Loss: 0.5374, Latency: 0.0287, Reward: 0.4097\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 2048\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7820\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6337\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5645\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5548\n",
            "    Mini-KD Loss: 0.5548, Latency: 0.0436, Reward: 0.3846\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 4\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 384\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7852\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6143\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5475\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5322\n",
            "    Mini-KD Loss: 0.5322, Latency: 0.0273, Reward: 0.4156\n",
            "Controller Epoch 1/10 | Avg Loss: 0.0277\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0138\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0078\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0046\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0027\n",
            "Controller Epoch 6/10 | Avg Loss: 0.0016\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0010\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0006\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0004\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0002\n",
            "\n",
            "Episode 7/10, Exploration Ratio: 0.70\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 4\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: gelu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7760\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6163\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5525\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5329\n",
            "    Mini-KD Loss: 0.5329, Latency: 0.0384, Reward: 0.4066\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8359\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7213\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5986\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5456\n",
            "    Mini-KD Loss: 0.5456, Latency: 0.0255, Reward: 0.4054\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 4\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 512\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8738\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7219\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6832\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.6087\n",
            "    Mini-KD Loss: 0.6087, Latency: 0.0137, Reward: 0.3625\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 4\n",
            "    num_attention_heads: 6\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 768\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9397\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7735\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7567\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.7431\n",
            "    Mini-KD Loss: 0.7431, Latency: 0.0098, Reward: 0.2427\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 6\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7811\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6478\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5820\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5467\n",
            "    Mini-KD Loss: 0.5467, Latency: 0.0441, Reward: 0.3913\n",
            "Controller Epoch 1/10 | Avg Loss: 0.0073\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0040\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0026\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0018\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0013\n",
            "Controller Epoch 6/10 | Avg Loss: 0.0009\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0006\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0005\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0003\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0002\n",
            "\n",
            "Episode 8/10, Exploration Ratio: 0.65\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8340\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7201\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6025\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5614\n",
            "    Mini-KD Loss: 0.5614, Latency: 0.0208, Reward: 0.3961\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1024\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8341\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7151\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5974\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5444\n",
            "    Mini-KD Loss: 0.5444, Latency: 0.0184, Reward: 0.4144\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 6\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9145\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7629\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7500\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.7180\n",
            "    Mini-KD Loss: 0.7180, Latency: 0.0117, Reward: 0.2637\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 2048\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7767\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6839\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5924\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5307\n",
            "    Mini-KD Loss: 0.5307, Latency: 0.0458, Reward: 0.4042\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 1024\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8558\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7199\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6849\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5885\n",
            "    Mini-KD Loss: 0.5885, Latency: 0.0161, Reward: 0.3773\n",
            "Controller Epoch 1/10 | Avg Loss: 0.0097\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0059\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0038\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0025\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0017\n",
            "Controller Epoch 6/10 | Avg Loss: 0.0011\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0007\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0005\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0003\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0002\n",
            "\n",
            "Episode 9/10, Exploration Ratio: 0.60\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7808\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6876\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5782\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5528\n",
            "    Mini-KD Loss: 0.5528, Latency: 0.0445, Reward: 0.3859\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 768\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7915\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6422\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5476\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5425\n",
            "    Mini-KD Loss: 0.5425, Latency: 0.0366, Reward: 0.3994\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 3\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 768\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8429\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6877\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5678\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5494\n",
            "    Mini-KD Loss: 0.5494, Latency: 0.0169, Reward: 0.4119\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 3\n",
            "    num_attention_heads: 6\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 1536\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8451\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7469\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6131\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5205\n",
            "    Mini-KD Loss: 0.5205, Latency: 0.0114, Reward: 0.4488\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 4\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 576\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7883\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6149\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5763\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5444\n",
            "    Mini-KD Loss: 0.5444, Latency: 0.0323, Reward: 0.4007\n",
            "Controller Epoch 1/10 | Avg Loss: 0.0244\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0119\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0083\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0061\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0045\n",
            "Controller Epoch 6/10 | Avg Loss: 0.0034\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0027\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0021\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0017\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0013\n",
            "\n",
            "Episode 10/10, Exploration Ratio: 0.55\n",
            "\n",
            "  Candidate 1/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 3072\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8404\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7117\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.6586\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5934\n",
            "    Mini-KD Loss: 0.5934, Latency: 0.0275, Reward: 0.3611\n",
            "\n",
            "  Candidate 2/5 Configuration:\n",
            "    num_hidden_layers: 12\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 768\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7783\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6368\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5595\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5318\n",
            "    Mini-KD Loss: 0.5318, Latency: 0.0427, Reward: 0.4050\n",
            "\n",
            "  Candidate 3/5 Configuration:\n",
            "    num_hidden_layers: 10\n",
            "    num_attention_heads: 12\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 3072\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.7770\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.6921\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5861\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5378\n",
            "    Mini-KD Loss: 0.5378, Latency: 0.0437, Reward: 0.3992\n",
            "\n",
            "  Candidate 4/5 Configuration:\n",
            "    num_hidden_layers: 6\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 384\n",
            "    intermediate_size: 384\n",
            "    hidden_act: silu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.9300\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7700\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.7482\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.6995\n",
            "    Mini-KD Loss: 0.6995, Latency: 0.0093, Reward: 0.2848\n",
            "\n",
            "  Candidate 5/5 Configuration:\n",
            "    num_hidden_layers: 4\n",
            "    num_attention_heads: 2\n",
            "    hidden_size: 768\n",
            "    intermediate_size: 2048\n",
            "    hidden_act: relu\n",
            "Epoch 1/4 | Avg Distillation Loss: 0.8412\n",
            "Epoch 2/4 | Avg Distillation Loss: 0.7357\n",
            "Epoch 3/4 | Avg Distillation Loss: 0.5812\n",
            "Epoch 4/4 | Avg Distillation Loss: 0.5082\n",
            "    Mini-KD Loss: 0.5082, Latency: 0.0161, Reward: 0.4509\n",
            "Controller Epoch 1/10 | Avg Loss: 0.0056\n",
            "Controller Epoch 2/10 | Avg Loss: 0.0026\n",
            "Controller Epoch 3/10 | Avg Loss: 0.0016\n",
            "Controller Epoch 4/10 | Avg Loss: 0.0010\n",
            "Controller Epoch 5/10 | Avg Loss: 0.0006\n",
            "Controller Epoch 6/10 | Avg Loss: 0.0004\n",
            "Controller Epoch 7/10 | Avg Loss: 0.0002\n",
            "Controller Epoch 8/10 | Avg Loss: 0.0001\n",
            "Controller Epoch 9/10 | Avg Loss: 0.0001\n",
            "Controller Epoch 10/10 | Avg Loss: 0.0001\n"
          ]
        }
      ],
      "source": [
        "# NAS Episodes\n",
        "for episode in range(M):\n",
        "    print(f\"\\nEpisode {episode+1}/{M}, Exploration Ratio: {epsilon:.2f}\")\n",
        "    num_random = int(epsilon * N)\n",
        "    num_controller = N - num_random\n",
        "    candidate_states = []\n",
        "\n",
        "    # Exploitation: Controller predicts high reward states\n",
        "    if num_controller > 0:\n",
        "        pool_states = [[np.random.randint(len(search_space[key])) for key in state_keys]\n",
        "                      for _ in range(pool_size)]\n",
        "        predicted_rewards = []\n",
        "        for state in pool_states:\n",
        "            with torch.no_grad():\n",
        "                reward_pred = controller(previous_best['state'], global_best['state'], state)\n",
        "            predicted_rewards.append(reward_pred.item())\n",
        "        indices = np.argsort(predicted_rewards)[-num_controller:]\n",
        "        for idx in indices:\n",
        "            candidate_states.append(pool_states[idx])\n",
        "\n",
        "    # Exploration: Random states\n",
        "    for _ in range(num_random):\n",
        "        state = [np.random.randint(len(search_space[key])) for key in state_keys]\n",
        "        candidate_states.append(state)\n",
        "\n",
        "    # Evaluate candidates\n",
        "    episode_rewards = []\n",
        "    for state_idx, state in enumerate(candidate_states):\n",
        "        config = {key: search_space[key][idx] for key, idx in zip(state_keys, state)}\n",
        "        print(f\"\\n  Candidate {state_idx+1}/{N} Configuration:\")\n",
        "        for key, value in config.items():\n",
        "            print(f\"    {key}: {value}\")\n",
        "        student_model = construct_student_model_from_config(config)\n",
        "        loss = mini_kd_trainer(proxy_data, teacher_model, student_model)\n",
        "        latency = get_latency(config)\n",
        "        reward = calculate_reward(loss, latency, teacher_latency)\n",
        "        episode_rewards.append(reward)\n",
        "        evaluated_states.append({'state': state, 'config': config, 'reward': reward})\n",
        "        print(f\"    Mini-KD Loss: {loss:.4f}, Latency: {latency:.4f}, Reward: {reward:.4f}\")\n",
        "\n",
        "    # Update best states\n",
        "    best_idx = np.argmax(episode_rewards)\n",
        "    episode_best_state = candidate_states[best_idx]\n",
        "    episode_best_reward = episode_rewards[best_idx]\n",
        "    previous_best = {'state': episode_best_state, 'reward': episode_best_reward}\n",
        "    if episode_best_reward > global_best['reward']:\n",
        "        global_best = {'state': episode_best_state, 'reward': episode_best_reward}\n",
        "\n",
        "    # Train controller\n",
        "    training_data = list(zip(candidate_states, episode_rewards))\n",
        "    training_data.append((global_best['state'], global_best['reward']))\n",
        "    training_data.append((previous_best['state'], previous_best['reward']))\n",
        "    train_controller(controller, controller_optimizer, training_data,\n",
        "                    previous_best['state'], global_best['state'])\n",
        "\n",
        "    # Decay exploration ratio\n",
        "    epsilon = max(epsilon_min, epsilon - epsilon_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZqduzGsFlOG",
        "outputId": "9d1b6c26-46e9-4f26-a93f-c16086c6c262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 3 Architectures:\n",
            "1. State: [1, 0, 1, 6, 1], Config: {'num_hidden_layers': 4, 'num_attention_heads': 2, 'hidden_size': 768, 'intermediate_size': 2048, 'hidden_act': 'relu'}, Reward: 0.4509\n",
            "2. State: [0, 3, 1, 5, 1], Config: {'num_hidden_layers': 3, 'num_attention_heads': 6, 'hidden_size': 768, 'intermediate_size': 1536, 'hidden_act': 'relu'}, Reward: 0.4488\n",
            "3. State: [0, 4, 1, 5, 1], Config: {'num_hidden_layers': 3, 'num_attention_heads': 12, 'hidden_size': 768, 'intermediate_size': 1536, 'hidden_act': 'relu'}, Reward: 0.4484\n"
          ]
        }
      ],
      "source": [
        "# Select top 3 architectures\n",
        "evaluated_states.sort(key=lambda x: x['reward'], reverse=True)\n",
        "top_3 = evaluated_states[:3]\n",
        "print(\"\\nTop 3 Architectures:\")\n",
        "for i, arch in enumerate(top_3, 1):\n",
        "    print(f\"{i}. State: {arch['state']}, Config: {arch['config']}, Reward: {arch['reward']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "187339b4a8d84bfe876a536bed761bff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2846cdfca175461ebfe3bbf94e1ad56e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d21619401a34192aaedcb6484f9c916": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dec93f7107c4a0482b55fc5b6b49075": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eba6c68faae34d6ea86049fbb47a29c8",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d86f20665d54d1089b63eeccece63d4",
            "value": 10000
          }
        },
        "5086ada160834376a44919a674efb0bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d86f20665d54d1089b63eeccece63d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b7ffcfcfc6646a3ab6ef8f149bca22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187339b4a8d84bfe876a536bed761bff",
            "placeholder": "â",
            "style": "IPY_MODEL_5086ada160834376a44919a674efb0bd",
            "value": "Map:â100%"
          }
        },
        "ccce72945cf8409e9b0587f62eb74a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2197858d207439abbe4eab1d290da7c",
            "placeholder": "â",
            "style": "IPY_MODEL_3d21619401a34192aaedcb6484f9c916",
            "value": "â10000/10000â[00:00&lt;00:00,â17762.41âexamples/s]"
          }
        },
        "dc1f6764876b43ceb68a6ab0a1e312bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b7ffcfcfc6646a3ab6ef8f149bca22f",
              "IPY_MODEL_3dec93f7107c4a0482b55fc5b6b49075",
              "IPY_MODEL_ccce72945cf8409e9b0587f62eb74a58"
            ],
            "layout": "IPY_MODEL_2846cdfca175461ebfe3bbf94e1ad56e"
          }
        },
        "eba6c68faae34d6ea86049fbb47a29c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2197858d207439abbe4eab1d290da7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
